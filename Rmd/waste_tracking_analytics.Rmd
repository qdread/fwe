---
title: "Intervention analysis: Waste tracking and analytics"
author: "Quentin D. Read"
date: "2/12/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE) # default to not show any of the code, messages, or warnings.
trunc_ellipsis <- function(x, n) if_else(nchar(x) < 30, x, paste(substr(x, 1, n), '...')) # Convenience function to cut off long labels.
```

# Summary

This is a financial cost - environmental benefit analysis of the adoption of food waste tracking & analytics systems (e.g., Leanpath) by all foodservice establishments in the USA with sufficient volume to justify the cost of implementation. This is a refinement of the analysis presented by ReFED in their "Roadmap" report.

**Once I get the results, insert a brief summary of them here.**

This is my first attempt at doing an intervention analysis of this kind. We can do a similar analysis for other food waste reduction interventions. Ultimately the goal would be to do multiple analyses and compare them.

# Outline of procedure

The analysis proceeds as follows:

1. Identify the industry groups that can adopt the intervention, using NAICS industry classifications and BEA codes that represent aggregations of NAICS industries. Identify the NAICS codes within each industry that do and do not include food establishments.
2. Total up the number of establishments and sales volume by size class within each industry, using SUSB data.
3. Impute any missing or censored values.
4. Use the predefined threshold of size class that can adopt the intervention, and the proportion of expenses by each industry that consists of food, to determine the number of establishments and proportion of receipts in each industry that will be affected by the intervention.
5. Use LAFA data to get a baseline waste rate for each of the target industries, which is a weighted average of the waste rates for each of the food groups the industry uses. (In this case it is a consumer level intervention so we will use the consumer level waste rates)
6. Use our assumptions of waste reduction rate achieved by the intervention to determine the dollar value of food waste averted by the intervention for each industry
7. Use the USEEIO 2012 model to find the environmental impact associated with this averted food waste.
8. Compare the impact averted (environmental benefit) with the annual cost of intervention (using the analysis done by Mary and multiplied by the number of establishments).
9. Repeat this analysis for different values within the upper and lower bounds of our assumptions, as an uncertainty analysis to see how much the results are affected by different assumptions.

# Assumptions

## Costs of establishment

The upper and lower bounds for annual cost per establishment of implementing waste tracking and analytics systems, rounded to the nearest thousand, are **\$9,000** and **\$20,000**. The uncertainty is due to uncertainty or variation in the following:

* initial cost of purchasing the system (assumed range \$2,000-\$5,000)
* annual licensing fee (assumed range \$1,000-\$2,000)
* cost of training employees (assumed range \$200-\$615, based on 10th to 90th percentile of employees' hourly wages in the foodservice industry)
* cost of daily setup time (assumed range \$4,106-\$7,659, again based on range of wages)
* cost of transaction time (assumed range \$411-\$766, again based on range of wages)
* cost of time spent reviewing records generated by analytics system (assumed range \$848-\$2,614, again based on range of wages)
* cost of time spent communicating and implementing changes suggested by analytics review (assumed range \$1,697-\$5,228, again based on range of wages)

The 10th-90th percentile range of wages for chefs and head cooks is fixed at \$12.66-\$39.01, and for food preparation workers at \$8.68-\$16.19 (*source: BLS, June 2019*)

Other potential sources of uncertainty which we could also incorporate into the range of possible cost values, but that are not currently incorporated, are: 

* the number of years over which to annualize the one-time purchase cost (currently fixed at 5)
* the interest rate used for annualization (fixed at 7%)
* the number of hours per year needed to train each employee (2)
* the number of employees requiring training per establishment (6)
* the daily setup time, in hours (1)
* the time per transaction, in seconds (12)
* the number of transactions per establishment per day (30)
* the number of hours per week needed to review records (1)
* the number of hours per week needed to communicate and implement changes (2)
* the benefits rate (31.4%; *source: BLS, June 2019*)

## Waste reduction

We define the effectiveness of the intervention as the percent reduction in food loss or waste achieved by implementing it. Thus, this number is very important for the final result. We are using a range of **40%-50%** reduction in kitchen waste. *Later we may also want to do the analysis using even lower numbers for waste reduction if this number is too optimistically high.* **Also I can't remember the source of this number. Is it from ReFED or Steve?**

We also need to know what percent of the food waste generated by foodservice establishments is made in the kitchen, relative to "plate waste" left over by the customers. We are using lower and upper bounds of **70%-97%** for this value, with a mode of **85%**, taken from three published studies conducted in Europe and the Middle East on foodservice establishments. This value is needed because baseline waste rate data from LAFA and other sources does not distinguish between cooking loss and plate loss at the consumer level &mdash; there is only one percentage loss value given.

## Minimum size of establishments that can adopt the intervention

Our data source for number of establishments by industry and by size class, Statistics of U.S. Businesses, aggregates firms by number of employees. We are assuming that any firm with **20 or more** employees can potentially adopt the intervention. *Later this assumption could be changed, or we could even specify different thresholds for different sectors or different industries within each sector. We are using a version of SUSB that I coarsened down to 4 size classes so that it could be compared to other datasets, but there are finer size classes available under 20 employees if we need them later.*

## Other assumptions

The decision of which BEA industries (and the component NAICS industries, for partially used industries) to include or exclude is an important assumption. See details below.

A further assumption is that the proportion of each industry that is affected by the intervention is related to the proportion of food expenses the industry pays for. That should be a tenable assumption in this case.

We also made a few assumptions in the process of imputing a few missing values, which could also be tested in the process of the uncertainty analysis. More detail on this is given below.

# Analysis

## 0. Gather needed data

We are using the following datasets and tables:

* Statistics of U.S. Businesses (SUSB) for 2012, for the numbers of employees, establishments, and total receipts by 6-digit NAICS code.
* Crosswalks assembled for previous work that link NAICS codes to the more aggregated BEA industries, and provide the proportion of expenses consisting of food for each BEA industry.
* USDA Loss-adjusted Food Availability (LAFA) data for each food type, to get baseline waste rates.
* The cost values and other individual values we assembled, described in the "Assumptions" section above.
* The USEEIO model requires the BEA input-output tables as well as satellite environmental impact tables for the relevant impact categories.

```{r load_data}
# Load packages and check whether this is being run locally or on rstudio server.
library(tidyverse)
library(reticulate)
library(directlabels)

is_local <- dir.exists('Q:/')
fp <- ifelse(is_local, 'Q:', '/nfs/qread-data')
fp_github <- file.path(ifelse(is_local, '~/Documents/GitHub', '~'))

# Load numbers of establishments by NAICS
susb_naics <- read_csv(file.path(fp, 'csv_exports/SUSB_NAICS_allvariables.csv'))
# Load the BEA code data (erroneously called NAICS)
bea_codes <- read_csv(file.path(fp, 'crossreference_tables/naics_crosswalk_final.csv'))
# Load NAICS BEA crosswalks
load(file.path(fp, 'crossreference_tables/NAICS_BEA_SCTG_crosswalk.RData'))
bea_naics <- read_csv(file.path(fp, 'crossreference_tables/BEA_NAICS07_NAICS12_crosswalk.csv'))
# Load LAFA
source(file.path(fp_github, 'fwe/read_data/read_lafa.r'))
lafa <- list(dairy, fat, fruit, grain, meat, sugar, veg)
```

```{r assumptions}
# For now, use the final output values from the cost sheet Mary made.
cost_range <- c(lower = 8749, upper = 20102) # Lower and upper bounds for annual cost, based on percentiles for employee hourly wages.

waste_reduction <- c(lower = 0.4, upper = 0.5, mode = 0.45) # Waste reduction achievable, rates from ReFED?

prop_kitchen_waste <- c(lower = 0.7, upper = 0.97, mode = 0.85) # Proportion kitchen waste (all other than plate waste), estimated from a few sources

# Overall waste reduction is going to be the proportion kitchen waste reduction you can get from WTA * the proportion of food waste that is in the kitchen versus "plate waste" generated by the customers

# This is going to be the same for all. We will use the "mode"
overall_waste_reduction <- prop_kitchen_waste["mode"] * waste_reduction["mode"]
```

## 1. Identify industries to target

I decided which BEA industries, of the 389 included in the detailed input-output table, to include in the foodservice classification. I classified foodservice industries into three broader sectors: restaurants, the tourism/hospitality industry, and institutional foodservice. **Note on scope: foodservice provided by state and federal governments, other than educational institutions, is not currently included in the scope of this analysis but could later be if we want.**

Each of the BEA industries is an aggregation of one or more 6-digit NAICS codes. Some of the BEA industries contain NAICS codes for which only part of the industries represent establishments with a food service component. For example, code 481000, "Air transportation," includes airport foodservice operations that could conceivably adopt waste tracking systems, but also a large amount of other non-food activity. The same is true for code 711200, "Spectator sports," which includes stadium foodservice.

The following table shows the industries belonging to each of the three sectors, along with the proportion of establishments (by NAICS code within industry) that have some foodservice component. (The table aggregates across all size classes but I will show data for each size class later.)

```{r subset industries}
codes_subset <- bea_codes %>% filter(stage %in% c('foodservice', 'institutional')) %>% select(BEA_389_code, BEA_389_def)

restaurants <- codes_subset$BEA_389_code[1:3]
tourism_hospitality <- codes_subset$BEA_389_code[c(4,6,9, 12, 14, 15, 16, 17, 18)] # Leave out movies and performances. Include air and ships.
institutions <- codes_subset$BEA_389_code[c(19, 20, 22:27)] # Leave out other educational services.

# 3 codes represent restaurants, 9 represent tourism/hospitality industry, 8 represent institutions that could adopt "WTA"

bea_to_use_df <- data.frame(BEA_Code = c(restaurants, tourism_hospitality, institutions),
                            sector = rep(c('restaurants', 'tourism and hospitality', 'institutions'), c(length(restaurants), length(tourism_hospitality), length(institutions))))

bea_naics_to_use <- bea_to_use_df %>% left_join(bea_naics)
# any(duplicated(bea_naics_to_use$related_2012_NAICS_6digit)) # There are no duplicates. 96 unique NAICS codes.

# Create subset with only food service.
susb_naics_foodservice <- bea_naics_to_use %>%
  select(BEA_Code, BEA_Title, sector, related_2012_NAICS_6digit) %>%
  rename(NAICS = related_2012_NAICS_6digit) %>%
  left_join(susb_naics)

# Flag rows that aren't going to be used (for example, freight transportation industries within transportation codes)
# Also remove campgrounds within the hospitality industry.
# unique(susb_naics_foodservice$`NAICS description`)
words_to_remove <- c('Freight', 'Nonscheduled', 'Air Traffic', 'Support Activities', 'Port', 'Cargo', 'Navigational', 'Towing', 'Packing', 'Campground')

susb_naics_foodservice <- susb_naics_foodservice %>%
  mutate(use = !grepl(paste(words_to_remove, collapse = '|'), `NAICS description`),
         `Size class` = factor(`Size class`, levels = c('fewer than 20', '20 to 99', '100 to 499', 'more than 500', 'total')))

# Find threshold of institution size to adopt WTA so that the volume of sales associated with establishments that adopt WTA 
# lines up with ReFED's assumptions that 80% of institutions and 15% of restaurants will adopt.

susb_food_sums <- susb_naics_foodservice %>% 
  group_by(sector, use, `Size class`) %>%
  summarize_at(vars(`No. firms`:`Total receipts`), sum) %>%
  filter(!is.na(`Size class`), !`Size class` %in% 'total')

# What are the proportions

susb_food_cumul_prop <- susb_food_sums %>%
  mutate_at(vars(`No. firms`:`Total receipts`), ~ cumsum(.x) / sum(.x))

# We can use 20 as the threshold. It can be altered.

size_classes_exclude <- expand_grid(sector = c('restaurants', 'tourism and hospitality', 'institutions'), `Size class` = levels(susb_food_sums$`Size class`)[1:4]) %>%
  mutate(exclude = `Size class` %in% c('fewer than 20'))

# Table to show in markdown with some summary info on foodservice establishment numbers.
susb_naics_foodservice %>% 
  group_by(sector, BEA_Code, BEA_Title, NAICS, `NAICS description`, use) %>%
  summarize_at(vars(`No. firms`:`Total receipts`), sum) %>%
  ungroup %>%
  select(sector, BEA_Title, NAICS, `NAICS description`, use, `No. establishments`) %>%
  setNames(c('sector','BEA','NAICS','description','foodservice','establishments')) %>%
  mutate(foodservice = if_else(foodservice, 'yes', 'no')) %>%
  filter(!is.na(description)) %>%
  print(n = nrow(.))
```

Side note: Not surprisingly, the number of firms as a function of size class is a "power law" relationship where there are many more small businesses than large ones with a more or less straight-line relationship on a logarithmic axis plot. Even so the total receipts per size class is dominated by the large firms.

```{r power law plots}
susb_food_sums %>%
  mutate(employees_per_firm = `No. employees`/`No. firms` ) %>%
  filter(use) %>%
  ggplot(aes(x = employees_per_firm, y = `No. firms`, color = sector)) + geom_line() + scale_x_log10(name = 'Number of employees per firm') + scale_y_log10(name = 'Number of firms in size class', breaks = c(1000, 10000, 100000), limits = c(1000, 400000)) + theme_minimal()

susb_food_sums %>%
  mutate(employees_per_firm = `No. employees`/`No. firms` ) %>%
  filter(use) %>%
  ggplot(aes(x = employees_per_firm, y = `Total receipts`, color = sector)) + geom_line() + scale_x_log10(name = 'Number of employees per firm') + scale_y_log10(name = 'Total receipts in size class') + theme_minimal()
```

## 2-4. Total up industry statistics, imputing where necessary

Next sum up all the industry statistics for all the NAICS codes that sharea a BEA code and size class. We will use this to determine the proportion of the total BEA industry volume that will be affected by implementing the intervention. If we make a logarithmic plot by industry showing the total receipts per firm, we should see a steady increase in every industry because the amount of money received by firm should increase with size. Any deviation means there are values that have been censored to protect companies' private data.

```{r sum by bea}
# Within each BEA code, get the percentage of total receipts that will be affected by WTA implementation
# All establishments with >20 employees, and also account for the fact that some tourism and hospitality BEA codes contain NAICS codes that aren't going to adopt WTA

susb_bea_food_sums <- susb_naics_foodservice %>%
  group_by(sector, BEA_Code, BEA_Title, use, `Size class`) %>%
  summarize_at(vars(`No. firms`:`Total receipts`), sum) %>%
  filter(!is.na(`Size class`), !`Size class` %in% 'total')
```

```{r diagnostic receipts plot}
library(directlabels)

susb_bea_food_sums %>%
  ungroup %>%
  filter(use) %>%
  mutate(receipts_per_firm = 1 + `Total receipts`/`No. firms`, BEA_Title = trunc_ellipsis(BEA_Title, 30)) %>%
  ggplot(aes(x = `Size class`, y = receipts_per_firm, group = BEA_Title)) + 
  geom_line(aes(color = BEA_Title)) + geom_dl(aes(label = BEA_Title), method = 'last.qp') +
  scale_y_log10(name = 'Receipts per firm') + theme_minimal() +
  theme(legend.position = 'none')
```

The plot shows that air and water transportation have $0 receipts for large firms. This is obviously a case of censored data. I used the values for smaller firms to find a regression relationship and impute the two missing values.

```{r imputation}
recbyestb <- susb_bea_food_sums %>%
  filter(use) %>%
  mutate(empl_per_firm = `No. employees`/`No. firms`,
         receipts_per_estb = `Total receipts`/`No. establishments`,
         receipts_per_firm = `Total receipts`/`No. firms`) 

recbyestb %>% 
  filter(BEA_Code %in% c('481000', '483000'), receipts_per_firm > 0) %>%
  ggplot(aes(x = empl_per_firm, y = receipts_per_firm, group = BEA_Title, color = BEA_Title)) + geom_point(size = 3) + scale_x_log10(name = 'Employees per firm') + scale_y_log10(name = 'Receipts per firm') + theme_minimal()
```

You can see that the total receipts per firm is essentially log-linear with number of employees per firm, so that is the relationship I used to impute the missing values. Unfortunately the number of employees was also censored for large water transportation firms so I used the number 700 taken from a similar industry (sightseeing transportation).

```{r imputation part 2}
# Impute the air transit number.
lm_air <- lm(log(receipts_per_firm) ~ log(empl_per_firm), data = recbyestb, subset = BEA_Title == 'Air transportation' & receipts_per_firm > 0)

predicted_air <- exp(predict(lm_air, newdata = recbyestb %>% filter(BEA_Title == 'Air transportation') %>% select(empl_per_firm)))
# predicted_air[4] # The imputed value for air transportation total receipts for firms with 500 or more employees.

# For water transportation, our problem is that we don't know either the average number of employees per firm or the receipts, the employees were
# also clearly censored wince only 202 employees for 11 firms is a lot less than 500 per firm. 
# Let's look for a similar industry to see whether we can get a number of employees per firm to use to impute.
lm_water <- lm(log(receipts_per_firm) ~ log(empl_per_firm), data = recbyestb, subset = BEA_Title == 'Water transportation' & receipts_per_firm > 0)

susb_bea_food_sums %>%
 mutate(empl_per_firm = `No. employees`/`No. firms`) %>%
 filter(`Size class` == 'more than 500')

# We will use the number from scenic transportation (700) which seems fairly conservative
# This number can be sampled from in an uncertainty analysis too.
recbyestb %>% filter(BEA_Title == 'Water transportation') %>% mutate(empl_per_firm = if_else(`Size class` == 'more than 500', 700, empl_per_firm)) %>% select(empl_per_firm)
predicted_water <- exp(predict(lm_water, newdata = recbyestb %>% filter(BEA_Title == 'Water transportation') %>% mutate(empl_per_firm = if_else(`Size class` == 'more than 500', 700, empl_per_firm)) %>% select(empl_per_firm)))
exp(predict(lm_water))

# Get the two imputed values

# Create new imputed dataset.
susb_bea_food_sums[susb_bea_food_sums$BEA_Code %in% c('481000', '483000') & susb_bea_food_sums$use & susb_bea_food_sums$`Size class` %in% 'more than 500', "Total receipts"] <- c(predicted_air[4], predicted_water[4])
```

With the missing values filled in we can now find the total establishments that can implement waste tracking within each BEA industry group, as well as the total proportion of industry sales volume that the food sales represent. These final values account for the threshold of 20 or more employees for adoption, for the subset of NAICS codes that represent establishments with a foodservice component, and the proportion expenses of the aggregated industries that consist of food versus other goods.

```{r get final proportions}
#### Here are the proportions of receipts that will be affected by adoption of WTA

susb_bea_proportion_affected <- susb_bea_food_sums %>% 
  ungroup %>%
  left_join(size_classes_exclude) %>%
  mutate(use = use & !exclude) %>%
  group_by(sector, BEA_Code, BEA_Title) %>%
  summarize(proportion_receipts_affected = sum(`Total receipts`[!exclude])/sum(`Total receipts`))

# Now multiply this by "proportion food" for each of the industries
prop_foods <- bea_codes %>%
  select(BEA_389_code, proportion_food) %>%
  rename(BEA_Code = BEA_389_code)

proportion_affected <- susb_bea_proportion_affected %>% left_join(prop_foods) %>% mutate(final_proportion = proportion_receipts_affected * proportion_food)

# We also need number of establishments affected so we can get the total annual cost
establishments_affected <- susb_bea_food_sums %>% 
  ungroup %>%
  left_join(size_classes_exclude) %>%
  mutate(use = use & !exclude) %>%
  group_by(sector, BEA_Code, BEA_Title, use) %>%
  summarize(establishments = sum(`No. establishments`))

```

The following two plots show the proportion of establishments that can adopt waste tracking by BEA industry, and the proportion of industry sales represented by food that is subject to waste reduction. In both cases, the threshold of $\geq20$ employees is used. In the case of establishments, only the establishments belonging to NAICS codes with a foodservice component are eligible to adopt the intervention. In the case of sales, the proportions are based on the proportion of expenses in each industry that consist of food. This assumes that if, say, 10% of an industry's purchases are food, then 10% of its sales are also food (i.e. that the relative value added is the same for the foodservice part of the industry as for any other goods it sells). This assumption seems fair enough but could be relaxed potentially, if we had any relevant data.

```{r barplots of establishment proportions}
ggplot(establishments_affected %>% ungroup %>% mutate(BEA_Title = trunc_ellipsis(BEA_Title, 30), BEA_Title = factor(BEA_Title, levels = unique(BEA_Title))), aes(y = establishments, x = BEA_Title, fill = sector, alpha = use)) + 
  geom_bar(position = 'stack', stat = 'identity') +
  scale_y_continuous(name = 'Number of establishments', limits = c(0, 3e5), expand = c(0,0)) +
  scale_fill_brewer(palette = 'Set2') +
  scale_alpha_manual(name = 'Can adopt', values = c(0.6, 1)) +
  theme_bw() +
  coord_flip() +
  theme(legend.position = 'bottom', legend.box = 'vertical') +
  ggtitle('Establishments that can adopt waste tracking')
```

The dark shaded part of the bars are the firms that have at least 20 employees and that are included in a NAICS code within the BEA aggregation that involves providing food.

```{r barplot of sales proportions}
# Make a barplot of the proportion of sales that is affected
ggplot(proportion_affected %>% ungroup %>% mutate(BEA_Title = trunc_ellipsis(BEA_Title, 30), BEA_Title = factor(BEA_Title, levels = unique(BEA_Title))), aes(y = final_proportion, x = BEA_Title, fill = sector)) + 
  geom_bar(stat = 'identity') +
  scale_y_continuous(name = 'Proportion of sales affected', limits = c(0,1), expand = c(0,0)) +
  scale_fill_brewer(palette = 'Set2') +
  theme_bw() +
  coord_flip() +
  theme(legend.position = 'bottom') +
  ggtitle('Proportion of sales influenced by waste tracking')
```

The proportion of sales is based on the proportion of expenses in the BEA group due to food, and based on firms that have at least 20 employees.


## 5. Get baseline waste rates for each industry with LAFA

**Note on scope: I am not including beverages in this analysis (alcoholic or otherwise). I don't think Leanpath or other waste trackers attempt to address beverage waste. If that is incorrect, I can add beverages back in.**

We know by what relative percentage the intervention can reduce waste within each industry (based on our assumptions), but we do not yet have the baseline waste rate which we would need to get the absolute value of the reduced waste for each industry. We can get the baseline rate using LAFA data but only if we run through a few conversion steps first. This uses the Quarterly Food-at-Home Price Database (QFAHPD) as the data source for relative prices of foods. This assumes that the relative prices are the same for wholesale and retail and also uses slightly older values from 2004-2010. However since all we care about is the relative prices, this should not be a big issue.

1. get relative % of purchases by BEA food growing/manufacturing industry group by \$ value for each food service industry group.
2. convert the relative % by \$ of BEA to relative % by \$ of QFAHPD using the BEA-QFAHPD crosswalk.
3. convert the $ values to relative weights using the QFAHPD value to weight conversion factors (price per weight).
4. convert the relative weights of QFAHPD food groups to relative weights of LAFA groups using the crosswalk table.

So the mapping needed is BEA --> QFAHPD --> LAFA. In cases where there are one-to-many mappings, the price or waste rate is taken as a simple unweighted average of the multiple categories that map back to a single one. This rough method (as compared to weighting things very precisely) is unlikely to greatly affect the result. In any case it is probably an improvement on using the very crude FAO categories as we did in the last publication.

The details of the mapping are in the underlying code that this PDF is rendered from, which pulls from two crosswalk tables that I manually created on 10 Feb 2020. It's uninteresting to describe here, but the final product is a baseline food waste rate for each of the foodservice industries, based on a weighted average of the food waste rates for the different foods it sells (converting money to weight back to money again).

```{r load data for waste rate calculations}
# BEA levels 1+3 to 4+6+7+8 is already subsetted from an older analysis I did.
food_U <- read.csv(file.path(fp, 'crossreference_tables/level13_to_level4678_inputs.csv'), row.names = 1, check.names = FALSE)

# Mapping will need to go bea --> qfahpd --> lafa
# Load the two necessary crosswalks.
bea2qfahpd <- read_csv(file.path(fp, 'crossreference_tables/bea_qfahpd_crosswalk.csv'))
qfahpd2lafa <- read_csv(file.path(fp, 'crossreference_tables/qfahpd_lafa_crosswalk.csv'))

# Also load the QFAHPD data so that we can get the prices.
qfahpd2 <- read_csv(file.path(fp, 'raw_data/USDA/QFAHPD/tidy_data/qfahpd2.csv'))

# Read the description of LAFA's nested category structure in.
lafa_struct <- read_csv(file.path(fp, 'crossreference_tables/lafa_category_structure.csv'))

# Get rid of unneeded rows and columns
food_U <- food_U[, proportion_affected$BEA_Code]
food_U <- food_U[rowSums(food_U) > 0, ]

# Beverage codes should be removed.
beveragecodes <- c('311920','311930','312110','312120','312130','312140')
food_U <- food_U[!row.names(food_U) %in% beveragecodes, ]
```


```{r convert BEA to QFAHPD to LAFA}
# Convert the comma-separated string columns to list columns.
bea2qfahpd <- bea2qfahpd %>%
  mutate(QFAHPD_code = strsplit(QFAHPD_code, ';'))
qfahpd2lafa <- qfahpd2lafa %>%
  mutate(LAFA_names = strsplit(LAFA_names, ';'))

# Do the mapping of food_U to QFAHPD codes.
food_U_QFAHPD <- food_U %>%
  mutate(BEA_389_code = row.names(food_U)) %>%
  pivot_longer(-BEA_389_code, names_to = 'BEA_recipient_code', values_to = 'monetary_flow') %>%
  left_join(bea2qfahpd %>% select(-BEA_389_def)) %>%
  group_by(BEA_389_code, BEA_recipient_code) %>%
  group_modify(~ data.frame(QFAHPD_code = .$QFAHPD_code[[1]], monetary_flow = .$monetary_flow/length(.$QFAHPD_code[[1]])))

# Now we have the use table where each BEA code has multiple rows for the different QFAHPD codes that make it up
# Create an aggregated version of QFAHPD to get the final price values for each code
# Weighted average across all market groups, years, and quarters
qfahpd_agg <- qfahpd2 %>%
  group_by(foodgroup) %>%
  summarize(price = weighted.mean(price, aggweight, na.rm = TRUE))

# Join the aggregated QFAHPD back up with its numerical codes and LAFA names
# Meanwhile correct a couple wrong names in the dairy category
qfahpd_agg <- qfahpd_agg %>% 
  mutate(foodgroup = gsub('Whole and 2%', 'Regular fat', foodgroup)) %>%
  left_join(qfahpd2lafa, by = c('foodgroup' = 'QFAHPD_name')) %>%
  mutate(QFAHPD_code = as.character(QFAHPD_code))

# Now join the aggregated QFAHPD with the food_U mapped to QFAHPD so that the total $ can be divided by $/weight to yield a weight (or mass).
# The units mass is in don't matter since they are all relative
food_U_LAFA <- food_U_QFAHPD %>%
  left_join(qfahpd_agg) %>%
  mutate(mass_flow = monetary_flow / price)

# For the unique LAFA names in the QFAHPD to LAFA mapping, extract the waste rates for 2012 or the closest year post-2012.
lafa_to_extract <- Reduce(union, qfahpd2lafa$LAFA_names)

# Split it up again by LAFA so that we can get the weights.
# Get only the columns we care about from each LAFA element in the list
# Then get the year closest to 2012
lafa_df <- lafa %>% 
  map_dfr(~ select(., Category, Year, Loss_at_consumer_level_Other__cooking_loss_and_uneaten_food__Percent, Consumer_weight_Lbs.year)) %>%
  rename(avoidable_consumer_loss = Loss_at_consumer_level_Other__cooking_loss_and_uneaten_food__Percent,
         consumer_weight = Consumer_weight_Lbs.year) %>%
  filter(!is.na(avoidable_consumer_loss)) %>%
  group_by(Category) %>%
  summarize(avoidable_consumer_loss = avoidable_consumer_loss[which.min(abs(Year-2012))],
            consumer_weight = consumer_weight[which.min(abs(Year-2012))])

# Use nested category structure to get weighted mean rates for the coarser LAFA groups
# for QFAHPD foods that do not resolve to the finest available level of LAFA
lafa_df <- lafa_df %>%
  left_join(lafa_struct, by = c('Category' = 'Food'))

lafa_group_rates <- map_dfr(1:4, function(i) lafa_df %>% 
  rename_(subgroup = paste0('subgroup', i)) %>%
  group_by(subgroup) %>% 
  summarize(avoidable_consumer_loss = weighted.mean(x = avoidable_consumer_loss, w = consumer_weight))) %>%
  filter(!is.na(subgroup))

# Use LAFA overall mean for prepared food in the "other" category
overall_mean <- with(lafa_df, weighted.mean(avoidable_consumer_loss, consumer_weight))

all_lafa_rates <- data.frame(Category = c(lafa_df$Category, lafa_group_rates$subgroup, 'prepared food'),
                             avoidable_consumer_loss = c(lafa_df$avoidable_consumer_loss, lafa_group_rates$avoidable_consumer_loss, overall_mean))
```

```{r plot histogram of LAFA}
# Rates of consumer-level "avoidable" waste by food group in LAFA (262 different foods)
ggplot(all_lafa_rates, aes(x = avoidable_consumer_loss/100)) + 
  geom_histogram() +
  theme_minimal() +
  scale_x_continuous(labels = scales::percent_format(accuracy = 1), name = 'Avoidable consumer food waste') +
  scale_y_continuous(expand = c(0,0.1))
```

**Insert a description of fig here**

```{r map mass flows to LAFA}
# Use the same pipe as last time to spread out the LAFA names over the rows

food_U_LAFA_spread <- food_U_LAFA %>%
  group_by(BEA_389_code, BEA_recipient_code, QFAHPD_code, price) %>%
  group_modify(~ data.frame(LAFA_name = .$LAFA_names[[1]], 
                            mass_flow = .$mass_flow/length(.$LAFA_names[[1]]),
                            monetary_flow = .$monetary_flow/length(.$LAFA_names[[1]]))) %>%
  left_join(all_lafa_rates, by = c('LAFA_name' = 'Category'))

# Join with the proportion of sales affected (also the same as proportion of mass affected) based on % sales that are food and the >20 employee threshold
# Then calculate the reduction in required mass flow for each food type, and then a weighted average to get the reduction in monetary flow needed
demand_change_fn <- function(W0, r, p) p * ((1 - W0) / (1 - (1 - r) * W0) - 1) + 1

food_U_LAFA_spread <- food_U_LAFA_spread %>% 
  left_join(proportion_affected, by = c('BEA_recipient_code' = 'BEA_Code')) %>%
  mutate(reduction_by_mass = demand_change_fn(W0 = avoidable_consumer_loss/100, r = overall_waste_reduction, p = proportion_receipts_affected),
         mass_flow_post_intervention = mass_flow * reduction_by_mass,
         monetary_flow_post_intervention = mass_flow_post_intervention * price)

# Sum up by old row and column index from the original food_U matrix, then reshape to make the same matrix.
food_U_postintervention_df <- food_U_LAFA_spread %>%
  group_by(BEA_389_code, BEA_recipient_code) %>%
  summarize(monetary_flow = sum(monetary_flow_post_intervention, na.rm = TRUE)) 

food_U_postintervention <- food_U_postintervention_df %>%
  pivot_wider(names_from = BEA_recipient_code, values_from = monetary_flow, values_fill = list(monetary_flow = 0)) %>%
  as.data.frame
row.names(food_U_postintervention) <- food_U_postintervention$BEA_389_code
food_U_postintervention <- food_U_postintervention[, !names(food_U_postintervention) %in% 'BEA_389_code']

# Make the row and column order the same as food_U
#row.names(food_U_postintervention) == row.names(food_U) # Good
food_U_postintervention <- food_U_postintervention[, names(food_U)]
```


## 6. Find total food purchases averted by foodservice industry

Once we have converted the monetary flows to mass flows, applied the waste rate changes, then converted them back to monetary flows, we have two matrices, a baseline and a post-intervention matrix. Each matrix has rows representing primary food growing and manufacturing industries, and columns representing foodservice industries. The values in the matrix are monetary flows, in millions of dollars, from each of the primary industries to the secondary industries in the baseline case and in the counterfactual case where the intervention was adopted, respectively.

Here I give the marginal column totals (how much each foodservice industry has to buy, total), and the marginal row totals (how much primary production is needed to supply the foodservice industry). The difference between the baseline and counterfactual represents the industry demand that is reduced by the successful waste reduction intervention.

## 7. Use input-output model to calculate impact averted by intervention

## 8. Compare cost of implementation with environmental benefit

## 9. Uncertainty analysis