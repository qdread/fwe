---
title: "Intervention analysis: Waste tracking and analytics (v1.1)"
author: "Quentin D. Read"
date: "February 19, 2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE) # default to not show any of the code, messages, or warnings.
trunc_ellipsis <- function(x, n) if_else(nchar(x) < 30, x, paste(substr(x, 1, n), '...')) # Convenience function to cut off long labels.
```

# Summary

**What?** This is a financial cost - environmental benefit analysis of the adoption of food waste tracking & analytics systems (e.g., Leanpath) by all foodservice establishments in the USA with sufficient volume to justify the cost of implementation. This is a refinement of the analysis presented by ReFED in their "Roadmap" report.

**How?** I used the cost data we assembled to get per-establishment costs, and a number of assumptions to determine the industry groups and numbers of eligible establishments. I converted each foodservice industry group's food purchases by dollar into mass units, estimated baseline and post-intervention waste, then converted back into dollar units. The EEIO model tells us the averted impact associated with the dollar value of averted purchases resulting from less waste in the foodservice industry.

**What was the result?** Given a roughly 40% reduction in foodservice waste, and a roughly 22-25% baseline waste rate, if all eligible establishments adopted waste tracking and analytics systems, the foodservice industry would have to buy 8-10% less raw materials. Because the EEIO model assumes linearity this would result in 8-10% decrease in environmental impact of the food service industry. The cost estimates are somewhat higher than ReFED's numbers so the cost per unit impact reduction is fairly high. If we only considered CO~2~ emission reduction it would be well short of what is considered a "good deal" (\$400 to \$1000 invested per tonne reduced, where $46/tonne is a decent benchmark). However there are so many other co-benefits that it may still be worth the investment.

This is my first attempt at doing an intervention analysis of this kind. We can do a similar analysis for other food waste reduction interventions. Ultimately the goal would be to do multiple analyses and compare them.

# Changelog

* **13 Feb 2020**: First draft, v1.0
* **19 Feb 2020**: Add estimates of environmental benefit offset by new equipment purchases, v1.1

# Outline of procedure

The analysis proceeds as follows:

1. Identify the industry groups that can adopt the intervention, using NAICS industry classifications and BEA codes that represent aggregations of NAICS industries. Identify the NAICS codes within each industry that do and do not include food establishments.
2. Total up the number of establishments and sales volume by size class within each industry, using SUSB data.
3. Impute any missing or censored values.
4. Use the predefined threshold of size class that can adopt the intervention, and the proportion of expenses by each industry that consists of food, to determine the number of establishments and proportion of receipts in each industry that will be affected by the intervention.
5. Use LAFA data to get a baseline waste rate for each of the target industries, which is a weighted average of the waste rates for each of the food groups the industry uses. (In this case it is a consumer level intervention so we will use the consumer level waste rates)
6. Use our assumptions of waste reduction rate achieved by the intervention to determine the dollar value of food waste averted by the intervention for each industry
7. Use the USEEIO 2012 model to find the environmental impact associated with this averted food waste.
8. Use the USEEIO 2012 model to find the environmental impact associated with equipment that must be purchased to implement the intervention, which will offset some of the environmental benefit.
8. Compare the net impact averted (environmental benefit) with the annual cost of intervention (using the analysis done by Mary and multiplied by the number of establishments).
9. Repeat this analysis for different values within the upper and lower bounds of our assumptions, as an uncertainty analysis to see how much the results are affected by different assumptions.

# Assumptions

## Costs of establishment

The upper and lower bounds for annual cost per establishment of implementing waste tracking and analytics systems, rounded to the nearest thousand, are **\$9,000** and **\$20,000**. The uncertainty is due to uncertainty or variation in the following:

* initial cost of purchasing the system (assumed range \$2,000-\$5,000)
* annual licensing fee (assumed range \$1,000-\$2,000)
* cost of training employees (assumed range \$200-\$615, based on 10th to 90th percentile of employees' hourly wages in the foodservice industry)
* cost of daily setup time (assumed range \$4,106-\$7,659, again based on range of wages)
* cost of transaction time (assumed range \$411-\$766, again based on range of wages)
* cost of time spent reviewing records generated by analytics system (assumed range \$848-\$2,614, again based on range of wages)
* cost of time spent communicating and implementing changes suggested by analytics review (assumed range \$1,697-\$5,228, again based on range of wages)

The 10th-90th percentile range of wages for chefs and head cooks is fixed at \$12.66-\$39.01, and for food preparation workers at \$8.68-\$16.19 (*source: BLS, June 2019*)

Other potential sources of uncertainty which we could also incorporate into the range of possible cost values, but that are not currently incorporated, are: 

* the number of years over which to annualize the one-time purchase cost (currently fixed at 5)
* the interest rate used for annualization (fixed at 7%)
* the number of hours per year needed to train each employee (2)
* the number of employees requiring training per establishment (6)
* the daily setup time, in hours (1)
* the time per transaction, in seconds (12)
* the number of transactions per establishment per day (30)
* the number of hours per week needed to review records (1)
* the number of hours per week needed to communicate and implement changes (2)
* the benefits rate (31.4%; *source: BLS, June 2019*)

## Waste reduction

We define the effectiveness of the intervention as the percent reduction in food loss or waste achieved by implementing it. Thus, this number is very important for the final result. We are using a range of **40%-50%** reduction in kitchen waste. *Later we may also want to do the analysis using even lower numbers for waste reduction if this number is too optimistically high.* **Also I can't remember the source of this number. Is it from ReFED or Steve?**

We also need to know what percent of the food waste generated by foodservice establishments is made in the kitchen, relative to "plate waste" left over by the customers. We are using lower and upper bounds of **70%-97%** for this value, with a mode of **85%**, taken from three published studies conducted in Europe and the Middle East on foodservice establishments. This value is needed because baseline waste rate data from LAFA and other sources does not distinguish between cooking loss and plate loss at the consumer level &mdash; there is only one percentage loss value given.

## Minimum size of establishments that can adopt the intervention

Our data source for number of establishments by industry and by size class, Statistics of U.S. Businesses, aggregates firms by number of employees. We are assuming that any firm with **20 or more** employees can potentially adopt the intervention. *Later this assumption could be changed, or we could even specify different thresholds for different sectors or different industries within each sector. We are using a version of SUSB that I coarsened down to 4 size classes so that it could be compared to other datasets, but there are finer size classes available under 20 employees if we need them later.*

## Other assumptions

We are assuming that the wholesale equipment costs account for roughly 25% of the initial cost of purchasing the waste tracking and analytics system.

The decision of which BEA industries (and the component NAICS industries, for partially used industries) to include or exclude is an important assumption. See details below.

A further assumption is that the proportion of each industry that is affected by the intervention is related to the proportion of food expenses the industry pays for. That should be a tenable assumption in this case.

We also made a few assumptions in the process of imputing a few missing values, which could also be tested in the process of the uncertainty analysis. More detail on this is given below.

# Analysis

## 0. Gather needed data

We are using the following datasets and tables:

* Statistics of U.S. Businesses (SUSB) for 2012, for the numbers of employees, establishments, and total receipts by 6-digit NAICS code.
* Crosswalks assembled for previous work that link NAICS codes to the more aggregated BEA industries, and provide the proportion of expenses consisting of food for each BEA industry.
* USDA Loss-adjusted Food Availability (LAFA) data for each food type, to get baseline waste rates by mass at the consumer level.
* USDA Quarterly Food-at-Home Price Database (QFAHPD), version 2, to get relative prices of foods, allowing conversion between monetary and physical flows.
* The cost values and other individual values we assembled, described in the "Assumptions" section above.
* The USEEIO model requires the Bureau of Economic Analysis' (BEA) input-output tables as well as satellite environmental impact tables for the relevant impact categories.

```{r load_data}
# Load packages and check whether this is being run locally or on rstudio server.
library(tidyverse)
library(reticulate)
library(directlabels)

is_local <- dir.exists('Q:/')
fp <- ifelse(is_local, 'Q:', '/nfs/qread-data')
fp_github <- file.path(ifelse(is_local, '~/Documents/GitHub', '~'))

# Load numbers of establishments by NAICS
susb_naics <- read_csv(file.path(fp, 'csv_exports/SUSB_NAICS_allvariables.csv'))
# Load the BEA code data (erroneously called NAICS)
bea_codes <- read_csv(file.path(fp, 'crossreference_tables/naics_crosswalk_final.csv'))
# Load NAICS BEA crosswalks
load(file.path(fp, 'crossreference_tables/NAICS_BEA_SCTG_crosswalk.RData'))
bea_naics <- read_csv(file.path(fp, 'crossreference_tables/BEA_NAICS07_NAICS12_crosswalk.csv'))
# Load LAFA
source(file.path(fp_github, 'fwe/read_data/read_lafa.r'))
lafa <- list(dairy, fat, fruit, grain, meat, sugar, veg)
```

```{r assumptions}
# For now, use the final output values from the cost sheet Mary made.
cost_range <- c(lower = 8749, upper = 20102) # Lower and upper bounds for annual cost, based on percentiles for employee hourly wages.

waste_reduction <- c(lower = 0.4, upper = 0.5, mode = 0.45) # Waste reduction achievable, rates from ReFED?

prop_kitchen_waste <- c(lower = 0.7, upper = 0.97, mode = 0.85) # Proportion kitchen waste (all other than plate waste), estimated from a few sources

equipment_costs <- c(lower = 375, upper = 800) # Lower and upper bounds for annual equipment costs, based on 25% estimate from Steve and our existing assumptions for lower and upper bounds for total annual cost of equipment and fees combined. (rounded)

# Overall waste reduction is going to be the proportion kitchen waste reduction you can get from WTA * the proportion of food waste that is in the kitchen versus "plate waste" generated by the customers

# This is going to be the same for all. We will use the "mode"
overall_waste_reduction <- prop_kitchen_waste["mode"] * waste_reduction["mode"]
```

## 1. Identify industries to target

I decided which BEA industries, of the 389 included in the detailed input-output table, to include in the foodservice classification. I classified foodservice industries into three broader sectors: restaurants, the tourism/hospitality industry, and institutional foodservice. **Note on scope: foodservice provided by state and federal governments, other than educational institutions, is not currently included in the scope of this analysis but could later be if we want.**

Each of the BEA industries is an aggregation of one or more 6-digit NAICS codes. Some of the BEA industries contain NAICS codes for which only part of the industries represent establishments with a food service component. For example, code 481000, "Air transportation," includes airport foodservice operations that could conceivably adopt waste tracking systems, but also a large amount of other non-food activity. The same is true for code 711200, "Spectator sports," which includes stadium foodservice.

The following table shows the industries belonging to each of the three sectors, along with the proportion of establishments (by NAICS code within industry) that have some foodservice component. (The table aggregates across all size classes but I will show data for each size class later.)

```{r subset industries}
codes_subset <- bea_codes %>% filter(stage %in% c('foodservice', 'institutional')) %>% select(BEA_389_code, BEA_389_def)

restaurants <- codes_subset$BEA_389_code[1:3]
tourism_hospitality <- codes_subset$BEA_389_code[c(4,6,9, 12, 14, 15, 16, 17, 18)] # Leave out movies and performances. Include air and ships.
institutions <- codes_subset$BEA_389_code[c(19, 20, 22:27)] # Leave out other educational services.

# 3 codes represent restaurants, 9 represent tourism/hospitality industry, 8 represent institutions that could adopt "WTA"

bea_to_use_df <- data.frame(BEA_Code = c(restaurants, tourism_hospitality, institutions),
                            sector = rep(c('restaurants', 'tourism and hospitality', 'institutions'), c(length(restaurants), length(tourism_hospitality), length(institutions))))

bea_naics_to_use <- bea_to_use_df %>% left_join(bea_naics)
# any(duplicated(bea_naics_to_use$related_2012_NAICS_6digit)) # There are no duplicates. 96 unique NAICS codes.

# Create subset with only food service.
susb_naics_foodservice <- bea_naics_to_use %>%
  select(BEA_Code, BEA_Title, sector, related_2012_NAICS_6digit) %>%
  rename(NAICS = related_2012_NAICS_6digit) %>%
  left_join(susb_naics)

# Flag rows that aren't going to be used (for example, freight transportation industries within transportation codes)
# Also remove campgrounds within the hospitality industry.
# unique(susb_naics_foodservice$`NAICS description`)
words_to_remove <- c('Freight', 'Nonscheduled', 'Air Traffic', 'Support Activities', 'Port', 'Cargo', 'Navigational', 'Towing', 'Packing', 'Campground')

susb_naics_foodservice <- susb_naics_foodservice %>%
  mutate(use = !grepl(paste(words_to_remove, collapse = '|'), `NAICS description`),
         `Size class` = factor(`Size class`, levels = c('fewer than 20', '20 to 99', '100 to 499', 'more than 500', 'total')))

# Find threshold of institution size to adopt WTA so that the volume of sales associated with establishments that adopt WTA 
# lines up with ReFED's assumptions that 80% of institutions and 15% of restaurants will adopt.

susb_food_sums <- susb_naics_foodservice %>% 
  group_by(sector, use, `Size class`) %>%
  summarize_at(vars(`No. firms`:`Total receipts`), sum) %>%
  filter(!is.na(`Size class`), !`Size class` %in% 'total')

# What are the proportions

susb_food_cumul_prop <- susb_food_sums %>%
  mutate_at(vars(`No. firms`:`Total receipts`), ~ cumsum(.x) / sum(.x))

# We can use 20 as the threshold. It can be altered.

size_classes_exclude <- expand_grid(sector = c('restaurants', 'tourism and hospitality', 'institutions'), `Size class` = levels(susb_food_sums$`Size class`)[1:4]) %>%
  mutate(exclude = `Size class` %in% c('fewer than 20'))

# Table to show in markdown with some summary info on foodservice establishment numbers.
susb_naics_foodservice %>% 
  group_by(sector, BEA_Code, BEA_Title, NAICS, `NAICS description`, use) %>%
  summarize_at(vars(`No. firms`:`Total receipts`), sum) %>%
  ungroup %>%
  select(sector, BEA_Title, NAICS, `NAICS description`, use, `No. establishments`) %>%
  setNames(c('sector','BEA','NAICS','description','foodservice','establishments')) %>%
  mutate(foodservice = if_else(foodservice, 'yes', 'no')) %>%
  filter(!is.na(description)) %>%
  print(n = nrow(.))
```

Side note: Not surprisingly, the number of firms as a function of size class is a "power law" relationship where there are many more small businesses than large ones with a more or less straight-line relationship on a logarithmic axis plot. Even so the total receipts per size class is dominated by the large firms.

```{r power law plots}
susb_food_sums %>%
  mutate(employees_per_firm = `No. employees`/`No. firms` ) %>%
  filter(use) %>%
  ggplot(aes(x = employees_per_firm, y = `No. firms`, color = sector)) + geom_line() + scale_x_log10(name = 'Number of employees per firm') + scale_y_log10(name = 'Number of firms in size class', breaks = c(1000, 10000, 100000), limits = c(1000, 400000)) + theme_minimal()

susb_food_sums %>%
  mutate(employees_per_firm = `No. employees`/`No. firms` ) %>%
  filter(use) %>%
  ggplot(aes(x = employees_per_firm, y = `Total receipts`, color = sector)) + geom_line() + scale_x_log10(name = 'Number of employees per firm') + scale_y_log10(name = 'Total receipts in size class') + theme_minimal()
```

## 2-4. Total up industry statistics, imputing where necessary

Next sum up all the industry statistics for all the NAICS codes that sharea a BEA code and size class. We will use this to determine the proportion of the total BEA industry volume that will be affected by implementing the intervention. If we make a logarithmic plot by industry showing the total receipts per firm, we should see a steady increase in every industry because the amount of money received by firm should increase with size. Any deviation means there are values that have been censored to protect companies' private data.

```{r sum by bea}
# Within each BEA code, get the percentage of total receipts that will be affected by WTA implementation
# All establishments with >20 employees, and also account for the fact that some tourism and hospitality BEA codes contain NAICS codes that aren't going to adopt WTA

susb_bea_food_sums <- susb_naics_foodservice %>%
  group_by(sector, BEA_Code, BEA_Title, use, `Size class`) %>%
  summarize_at(vars(`No. firms`:`Total receipts`), sum) %>%
  filter(!is.na(`Size class`), !`Size class` %in% 'total')
```

```{r diagnostic receipts plot}
library(directlabels)

susb_bea_food_sums %>%
  ungroup %>%
  filter(use) %>%
  mutate(receipts_per_firm = 1 + `Total receipts`/`No. firms`, BEA_Title = trunc_ellipsis(BEA_Title, 30)) %>%
  ggplot(aes(x = `Size class`, y = receipts_per_firm, group = BEA_Title)) + 
  geom_line(aes(color = BEA_Title)) + geom_dl(aes(label = BEA_Title), method = 'last.qp') +
  scale_y_log10(name = 'Receipts per firm') + theme_minimal() +
  theme(legend.position = 'none')
```

The plot shows that air and water transportation have $0 receipts for large firms. This is obviously a case of censored data. I used the values for smaller firms to find a regression relationship and impute the two missing values.

```{r imputation}
recbyestb <- susb_bea_food_sums %>%
  filter(use) %>%
  mutate(empl_per_firm = `No. employees`/`No. firms`,
         receipts_per_estb = `Total receipts`/`No. establishments`,
         receipts_per_firm = `Total receipts`/`No. firms`) 

recbyestb %>% 
  filter(BEA_Code %in% c('481000', '483000'), receipts_per_firm > 0) %>%
  ggplot(aes(x = empl_per_firm, y = receipts_per_firm, group = BEA_Title, color = BEA_Title)) + geom_point(size = 3) + scale_x_log10(name = 'Employees per firm') + scale_y_log10(name = 'Receipts per firm') + theme_minimal()
```

You can see that the total receipts per firm is essentially log-linear with number of employees per firm, so that is the relationship I used to impute the missing values. Unfortunately the number of employees was also censored for large water transportation firms so I used the number 700 taken from a similar industry (sightseeing transportation).

```{r imputation part 2}
# Impute the air transit number.
lm_air <- lm(log(receipts_per_firm) ~ log(empl_per_firm), data = recbyestb, subset = BEA_Title == 'Air transportation' & receipts_per_firm > 0)

predicted_air <- exp(predict(lm_air, newdata = recbyestb %>% filter(BEA_Title == 'Air transportation') %>% select(empl_per_firm)))
# predicted_air[4] # The imputed value for air transportation total receipts for firms with 500 or more employees.

# For water transportation, our problem is that we don't know either the average number of employees per firm or the receipts, the employees were
# also clearly censored wince only 202 employees for 11 firms is a lot less than 500 per firm. 
# Let's look for a similar industry to see whether we can get a number of employees per firm to use to impute.
lm_water <- lm(log(receipts_per_firm) ~ log(empl_per_firm), data = recbyestb, subset = BEA_Title == 'Water transportation' & receipts_per_firm > 0)

susb_bea_food_sums %>%
 mutate(empl_per_firm = `No. employees`/`No. firms`) %>%
 filter(`Size class` == 'more than 500')

# We will use the number from scenic transportation (700) which seems fairly conservative
# This number can be sampled from in an uncertainty analysis too.
recbyestb %>% filter(BEA_Title == 'Water transportation') %>% mutate(empl_per_firm = if_else(`Size class` == 'more than 500', 700, empl_per_firm)) %>% select(empl_per_firm)
predicted_water <- exp(predict(lm_water, newdata = recbyestb %>% filter(BEA_Title == 'Water transportation') %>% mutate(empl_per_firm = if_else(`Size class` == 'more than 500', 700, empl_per_firm)) %>% select(empl_per_firm)))
exp(predict(lm_water))

# Get the two imputed values

# Create new imputed dataset.
susb_bea_food_sums[susb_bea_food_sums$BEA_Code %in% c('481000', '483000') & susb_bea_food_sums$use & susb_bea_food_sums$`Size class` %in% 'more than 500', "Total receipts"] <- c(predicted_air[4], predicted_water[4])
```

With the missing values filled in we can now find the total establishments that can implement waste tracking within each BEA industry group, as well as the total proportion of industry sales volume that the food sales represent. These final values account for the threshold of 20 or more employees for adoption, for the subset of NAICS codes that represent establishments with a foodservice component, and the proportion expenses of the aggregated industries that consist of food versus other goods.

```{r get final proportions}
#### Here are the proportions of receipts that will be affected by adoption of WTA

susb_bea_proportion_affected <- susb_bea_food_sums %>% 
  ungroup %>%
  left_join(size_classes_exclude) %>%
  mutate(use = use & !exclude) %>%
  group_by(sector, BEA_Code, BEA_Title) %>%
  summarize(proportion_receipts_affected = sum(`Total receipts`[!exclude])/sum(`Total receipts`))

# Now multiply this by "proportion food" for each of the industries
prop_foods <- bea_codes %>%
  select(BEA_389_code, proportion_food) %>%
  rename(BEA_Code = BEA_389_code)

proportion_affected <- susb_bea_proportion_affected %>% left_join(prop_foods) %>% mutate(final_proportion = proportion_receipts_affected * proportion_food)

# We also need number of establishments affected so we can get the total annual cost
establishments_affected <- susb_bea_food_sums %>% 
  ungroup %>%
  left_join(size_classes_exclude) %>%
  mutate(use = use & !exclude) %>%
  group_by(sector, BEA_Code, BEA_Title, use) %>%
  summarize(establishments = sum(`No. establishments`))

```

The following two plots show the proportion of establishments that can adopt waste tracking by BEA industry, and the proportion of industry sales represented by food that is subject to waste reduction. In both cases, the threshold of $\geq20$ employees is used. In the case of establishments, only the establishments belonging to NAICS codes with a foodservice component are eligible to adopt the intervention. In the case of sales, the proportions are based on the proportion of expenses in each industry that consist of food. This assumes that if, say, 10% of an industry's purchases are food, then 10% of its sales are also food (i.e. that the relative value added is the same for the foodservice part of the industry as for any other goods it sells). This assumption seems fair enough but could be relaxed potentially, if we had any relevant data.

```{r barplots of establishment proportions}
ggplot(establishments_affected %>% ungroup %>% mutate(BEA_Title = trunc_ellipsis(BEA_Title, 30), BEA_Title = factor(BEA_Title, levels = unique(BEA_Title))), aes(y = establishments, x = BEA_Title, fill = sector, alpha = use)) + 
  geom_bar(position = 'stack', stat = 'identity') +
  scale_y_continuous(name = 'Number of establishments', limits = c(0, 3e5), expand = c(0,0)) +
  scale_fill_brewer(palette = 'Set2') +
  scale_alpha_manual(name = 'Can adopt', values = c(0.6, 1)) +
  theme_bw() +
  coord_flip() +
  theme(legend.position = 'bottom', legend.box = 'vertical') +
  ggtitle('Establishments that can adopt waste tracking')
```

The dark shaded part of the bars are the firms that have at least 20 employees and that are included in a NAICS code within the BEA aggregation that involves providing food.

```{r barplot of sales proportions}
# Make a barplot of the proportion of sales that is affected
ggplot(proportion_affected %>% ungroup %>% mutate(BEA_Title = trunc_ellipsis(BEA_Title, 30), BEA_Title = factor(BEA_Title, levels = unique(BEA_Title))), aes(y = final_proportion, x = BEA_Title, fill = sector)) + 
  geom_bar(stat = 'identity') +
  scale_y_continuous(name = 'Proportion of sales affected', limits = c(0,1), expand = c(0,0)) +
  scale_fill_brewer(palette = 'Set2') +
  theme_bw() +
  coord_flip() +
  theme(legend.position = 'bottom') +
  ggtitle('Proportion of sales influenced by waste tracking')
```

The proportion of sales is based on the proportion of expenses in the BEA group due to food, and based on firms that have at least 20 employees.


## 5. Get baseline waste rates for each industry with LAFA

**Note on scope: I am not including beverages in this analysis (alcoholic or otherwise). I don't think Leanpath or other waste trackers attempt to address beverage waste. If that is incorrect, I can add beverages back in.**

We know by what relative percentage the intervention can reduce waste within each industry (based on our assumptions), but we do not yet have the baseline waste rate which we would need to get the absolute value of the reduced waste for each industry. We can get the baseline rate using LAFA data but only if we run through a few conversion steps first. This uses the BEA input-output tables as the data source for the monetary flows from primary food industries (agriculture and manufacturing/processing) to the secondary foodservice industries, and the Quarterly Food-at-Home Price Database (QFAHPD) as the data source for relative prices of foods. This assumes that the relative prices are the same for wholesale and retail and also uses slightly older values from 2004-2010. However since all we care about is the relative prices, this should not be a big issue.

The necessary steps are to:

1. get relative % of purchases by BEA food growing/manufacturing industry group by \$ value for each foodservice industry group.
2. convert the relative % by \$ of BEA to relative % by \$ of QFAHPD using the BEA-QFAHPD crosswalk.
3. convert the $ values to relative weights using the QFAHPD value to weight conversion factors (price per weight).
4. convert the relative weights of QFAHPD food groups to relative weights of LAFA groups using the crosswalk table.

So the mapping needed is BEA --> QFAHPD --> LAFA. In cases where there are one-to-many mappings, the price or waste rate is taken as a simple unweighted average of the multiple categories that map back to a single one. This rough method (as compared to weighting things very precisely) is unlikely to greatly affect the result. In any case it is probably an improvement on using the very crude FAO categories as we did in the last publication.

The details of the mapping are in the underlying code that this PDF is rendered from, which pulls from two crosswalk tables that I manually created on 10 Feb 2020. It's uninteresting to describe here, but the final product is a baseline food waste rate for each of the foodservice industries, based on a weighted average of the food waste rates for the different foods it sells (converting money to weight back to money again).

```{r load data for waste rate calculations}
# BEA levels 1+3 to 4+6+7+8 is already subsetted from an older analysis I did.
food_U <- read.csv(file.path(fp, 'crossreference_tables/level13_to_level4678_inputs.csv'), row.names = 1, check.names = FALSE)

# Mapping will need to go bea --> qfahpd --> lafa
# Load the two necessary crosswalks.
bea2qfahpd <- read_csv(file.path(fp, 'crossreference_tables/bea_qfahpd_crosswalk.csv'))
qfahpd2lafa <- read_csv(file.path(fp, 'crossreference_tables/qfahpd_lafa_crosswalk.csv'))

# Also load the QFAHPD data so that we can get the prices.
qfahpd2 <- read_csv(file.path(fp, 'raw_data/USDA/QFAHPD/tidy_data/qfahpd2.csv'))

# Read the description of LAFA's nested category structure in.
lafa_struct <- read_csv(file.path(fp, 'crossreference_tables/lafa_category_structure.csv'))

# Demand codes table to convert 6 digit codes to the ones used by USEEIO
all_codes <- read_csv(file.path(fp, 'crossreference_tables/all_codes.csv'))

# Get rid of unneeded rows and columns
food_U <- food_U[, proportion_affected$BEA_Code]
food_U <- food_U[rowSums(food_U) > 0, ]

# Beverage codes should be removed.
beveragecodes <- c('311920','311930','312110','312120','312130','312140')
food_U <- food_U[!row.names(food_U) %in% beveragecodes, ]
```


```{r map BEA to QFAHPD}
# Convert the comma-separated string columns to list columns.
bea2qfahpd <- bea2qfahpd %>%
  mutate(QFAHPD_code = strsplit(QFAHPD_code, ';'))
qfahpd2lafa <- qfahpd2lafa %>%
  mutate(LAFA_names = strsplit(LAFA_names, ';'))

# Do the mapping of food_U to QFAHPD codes.
food_U_QFAHPD <- food_U %>%
  mutate(BEA_389_code = row.names(food_U)) %>%
  pivot_longer(-BEA_389_code, names_to = 'BEA_recipient_code', values_to = 'monetary_flow') %>%
  left_join(bea2qfahpd %>% select(-BEA_389_def)) %>%
  group_by(BEA_389_code, BEA_recipient_code) %>%
  group_modify(~ data.frame(QFAHPD_code = .$QFAHPD_code[[1]], monetary_flow = .$monetary_flow/length(.$QFAHPD_code[[1]])))

# Now we have the use table where each BEA code has multiple rows for the different QFAHPD codes that make it up
# Create an aggregated version of QFAHPD to get the final price values for each code
# Weighted average across all market groups, years, and quarters
qfahpd_agg <- qfahpd2 %>%
  group_by(foodgroup) %>%
  summarize(price = weighted.mean(price, aggweight, na.rm = TRUE))

# Join the aggregated QFAHPD back up with its numerical codes and LAFA names
# Meanwhile correct a couple wrong names in the dairy category
qfahpd_agg <- qfahpd_agg %>% 
  mutate(foodgroup = gsub('Whole and 2%', 'Regular fat', foodgroup)) %>%
  left_join(qfahpd2lafa, by = c('foodgroup' = 'QFAHPD_name')) %>%
  mutate(QFAHPD_code = as.character(QFAHPD_code))

# Now join the aggregated QFAHPD with the food_U mapped to QFAHPD so that the total $ can be divided by $/weight to yield a weight (or mass).
# The units mass is in don't matter since they are all relative
food_U_LAFA <- food_U_QFAHPD %>%
  left_join(qfahpd_agg) %>%
  mutate(mass_flow = monetary_flow / price)
```

```{r get all LAFA rates including averages of lower level groups}
# For the unique LAFA names in the QFAHPD to LAFA mapping, extract the waste rates for 2012 or the closest year post-2012.
lafa_to_extract <- Reduce(union, qfahpd2lafa$LAFA_names)

# Split it up again by LAFA so that we can get the weights.
# Get only the columns we care about from each LAFA element in the list
# Then get the year closest to 2012
lafa_df <- lafa %>% 
  map_dfr(~ select(., Category, Year, Loss_at_consumer_level_Other__cooking_loss_and_uneaten_food__Percent, Consumer_weight_Lbs.year)) %>%
  rename(avoidable_consumer_loss = Loss_at_consumer_level_Other__cooking_loss_and_uneaten_food__Percent,
         consumer_weight = Consumer_weight_Lbs.year) %>%
  filter(!is.na(avoidable_consumer_loss)) %>%
  group_by(Category) %>%
  summarize(avoidable_consumer_loss = avoidable_consumer_loss[which.min(abs(Year-2012))],
            consumer_weight = consumer_weight[which.min(abs(Year-2012))])

# Use nested category structure to get weighted mean rates for the coarser LAFA groups
# for QFAHPD foods that do not resolve to the finest available level of LAFA
lafa_df <- lafa_df %>%
  left_join(lafa_struct, by = c('Category' = 'Food'))

lafa_group_rates <- map_dfr(1:4, function(i) lafa_df %>% 
  rename_(subgroup = paste0('subgroup', i)) %>%
  group_by(subgroup) %>% 
  summarize(avoidable_consumer_loss = weighted.mean(x = avoidable_consumer_loss, w = consumer_weight))) %>%
  filter(!is.na(subgroup))

# Use LAFA overall mean for prepared food in the "other" category
overall_mean <- with(lafa_df, weighted.mean(avoidable_consumer_loss, consumer_weight))

all_lafa_rates <- data.frame(Category = c(lafa_df$Category, lafa_group_rates$subgroup, 'prepared food'),
                             avoidable_consumer_loss = c(lafa_df$avoidable_consumer_loss, lafa_group_rates$avoidable_consumer_loss, overall_mean))
```

```{r plot histogram of LAFA}
# Rates of consumer-level "avoidable" waste by food group in LAFA (262 different foods)
ggplot(all_lafa_rates, aes(x = avoidable_consumer_loss/100)) + 
  geom_histogram() +
  theme_minimal() +
  scale_x_continuous(labels = scales::percent_format(accuracy = 1), name = 'Avoidable consumer food waste') +
  scale_y_continuous(expand = c(0,0.1))
```

This is a histogram of the "avoidable share" of consumer food waste for all ~250 foods included in LAFA. This includes cooking loss and uneaten food but does not include inedible parts such as bones or rinds. Once we have converted the monetary flows to mass flows for each foodservice industry, we can use these individual waste rates to get a weighted average waste rate for each individual industry.

```{r map mass flows to LAFA and convert back to monetary}
# Use the same pipe as last time to spread out the LAFA names over the rows

food_U_LAFA_spread <- food_U_LAFA %>%
  group_by(BEA_389_code, BEA_recipient_code, QFAHPD_code, price) %>%
  group_modify(~ data.frame(LAFA_name = .$LAFA_names[[1]], 
                            mass_flow = .$mass_flow/length(.$LAFA_names[[1]]),
                            monetary_flow = .$monetary_flow/length(.$LAFA_names[[1]]))) %>%
  left_join(all_lafa_rates, by = c('LAFA_name' = 'Category'))

# Join with the proportion of sales affected (also the same as proportion of mass affected) based on % sales that are food and the >20 employee threshold
# Then calculate the reduction in required mass flow for each food type, and then a weighted average to get the reduction in monetary flow needed
demand_change_fn <- function(W0, r, p) p * ((1 - W0) / (1 - (1 - r) * W0) - 1) + 1

food_U_LAFA_spread <- food_U_LAFA_spread %>% 
  left_join(proportion_affected, by = c('BEA_recipient_code' = 'BEA_Code')) %>%
  mutate(reduction_by_mass = demand_change_fn(W0 = avoidable_consumer_loss/100, r = overall_waste_reduction, p = proportion_receipts_affected),
         mass_flow_post_intervention = mass_flow * reduction_by_mass,
         monetary_flow_post_intervention = mass_flow_post_intervention * price)

# Sum up by old row and column index from the original food_U matrix, then reshape to make the same matrix.
food_U_postintervention_df <- food_U_LAFA_spread %>%
  group_by(BEA_389_code, BEA_recipient_code) %>%
  summarize(monetary_flow = sum(monetary_flow_post_intervention, na.rm = TRUE)) 

food_U_postintervention <- food_U_postintervention_df %>%
  pivot_wider(names_from = BEA_recipient_code, values_from = monetary_flow, values_fill = list(monetary_flow = 0)) %>%
  as.data.frame
row.names(food_U_postintervention) <- food_U_postintervention$BEA_389_code
food_U_postintervention <- food_U_postintervention[, !names(food_U_postintervention) %in% 'BEA_389_code']

# Make the row and column order the same as food_U
#row.names(food_U_postintervention) == row.names(food_U) # Good
food_U_postintervention <- food_U_postintervention[, names(food_U)]
```

```{r summarize monetary and mass flow changes}
# The % of waste reduced by mass won't be exactly equivalent to the % of waste reduced by $.

postintervention <- food_U_postintervention_df %>%
  group_by(BEA_recipient_code) %>%
  summarize(monetary_flow_postintervention = sum(monetary_flow))

preintervention <- data.frame(BEA_recipient_code = names(food_U), monetary_flow_preintervention = colSums(food_U))

monetary_byintervention <- left_join(postintervention, preintervention) %>%
  mutate(reduction_bydollar = 1 - monetary_flow_postintervention / monetary_flow_preintervention) 

mass_byintervention <- food_U_LAFA_spread %>%
  group_by(BEA_recipient_code) %>%
  summarize(mass_flow_preintervention = sum(mass_flow, na.rm = TRUE),
            mass_flow_postintervention = sum(mass_flow_post_intervention, na.rm = TRUE)) %>%
  mutate(reduction_bymass = 1 - mass_flow_postintervention / mass_flow_preintervention) 

rate_changes <- left_join(mass_byintervention, monetary_byintervention)

# A better way is to phrase it as reducing the operating costs (purchases of raw materials) by the affected industries
# Since we have a waste rate reduction by mass, convert it back to waste rate reduction by money and get the purchasing rate reduction from each of the industries that supply the final foodservice industries.

# Calculate pre and post incoming monetary food flow in millions of dollars
total_prepost <- data.frame(BEA_Code = names(food_U),
                            food_purchases_baseline = colSums(food_U),
                            food_purchases_postintervention = colSums(food_U_postintervention)) %>%
  mutate(reduction = food_purchases_baseline - food_purchases_postintervention)
```

## 6. Find total food purchases averted by foodservice industry

Once we have converted the monetary flows to mass flows, applied the waste rate changes, then converted them back to monetary flows, we have two matrices, a baseline and a post-intervention matrix. Each matrix has rows representing primary food growing and manufacturing industries, and columns representing foodservice industries. The values in the matrix are monetary flows, in millions of dollars, from each of the primary industries to the secondary industries in the baseline case and in the counterfactual case where the intervention was adopted, respectively.

Here I give the marginal column totals (how much each foodservice industry has to buy, total), and the marginal row totals (how much primary production is needed to supply the foodservice industry). The difference between the baseline and counterfactual represents the industry demand that is reduced by the successful waste reduction intervention. The value for waste reduction is currently based on a point estimate for the effectiveness of the reduction and a single value for the proportion kitchen waste across all industries (45% waste reduction and 85% of foodservice waste generated in the kitchen; $0.45 \times 0.85 = 0.3825$). Note that all units are in millions of USD.

```{r baseline waste rate for each foodservice industry}
# Weighted mean of waste rate by mass flow for each foodservice industry = final waste rate for the industries!
baseline_waste_foodservice <- food_U_LAFA_spread %>%
  group_by(BEA_recipient_code) %>%
  summarize(avoidable_consumer_loss = weighted.mean(x = avoidable_consumer_loss, w = mass_flow, na.rm = TRUE))

# Join up the names of the BEA codes and print the table of values
baseline_waste_foodservice <- baseline_waste_foodservice %>% 
  left_join(codes_subset, by = c('BEA_recipient_code' = 'BEA_389_code')) %>%
  setNames(c('BEA_Code', 'baseline', 'BEA_Title')) %>%
  left_join(bea_to_use_df)

baseline_waste_foodservice <- baseline_waste_foodservice %>%
  left_join(proportion_affected %>% select(BEA_Code, proportion_receipts_affected, proportion_food)) %>%
  left_join(total_prepost)

baseline_waste_foodservice %>%
  mutate(BEA_Title = trunc_ellipsis(BEA_Title, 30)) %>%
  select(sector, BEA_Title, baseline, food_purchases_baseline, food_purchases_postintervention, reduction) %>%
  rename(baseline_waste_percent = baseline) %>%
  arrange(sector) %>%
  print(n = nrow(.))
```

```{r plot of monetary reduction versus mass reduction}
# Proportion reduction in monetary units is systematically higher than in mass units but 
ggplot(rate_changes) +
  geom_point(aes(x = reduction_bymass, y = reduction_bydollar)) +
  geom_abline(slope=1, linetype = 'dotted') +
  annotate(geom = 'text', x = 0.095, y = 0.094, label = "1:1 line", angle = 45) +
  theme_minimal() +
  labs(x = 'proportion reduction by mass', y = 'proportion reduction by $') +
  theme(aspect.ratio = 1)
```

Within each foodservice industry, the proportional waste reduction in monetary units tends to be slightly higher than the proportion reduction in mass units but the difference is quite small (for example, 8% reduction in mass waste equals about 9% reduction in monetary waste). Therefore the conversion and weighting procedure has a relatively small effect on the final result. 

This table shows the marginal row totals, by primary food industry, of value sold to the secondary foodservice industries, in millions of dollars.

```{r marginal row totals to show waste reduction by food type}
# Above they were marginal column totals for the recipient industries
# Phrase it also as marginal row totals for the food types

reduction_byfoodtype <- data.frame(BEA_Code = row.names(food_U),
                                   cost_averted = rowSums(food_U) - rowSums(food_U_postintervention))

# For display purposes join this with the name of the food so that we can see the names of the rows being totaled up.
bea_codes %>%
  select(BEA_389_code, BEA_389_def) %>%
  rename(BEA_Code = BEA_389_code, BEA_Title = BEA_389_def) %>%
  right_join(reduction_byfoodtype) %>%
  mutate(BEA_Title = trunc_ellipsis(BEA_Title, 30)) %>%
  print(n = nrow(.))
```


## 7. Use input-output model to calculate impact averted by intervention

Now that we have the monetary flows representing purchases from primary food industries to secondary food industries for both the baseline and counterfactual (post-intervention) case, we can run the USEEIO environmentally extended input-output model on each of these to get the baseline environmental impact and the amount of impact averted by the intervention. We treat the intermediate demand marginal sums as if it were a final demand vector to determine the impact it represents.

```{r run EEIO}
# Simply enough, just run it on the difference in the two rowSums for pre and post intervention
# This represents final operating costs of the industries, as if it were final consumer demand
if (!is_local) use_python('/usr/bin/python3')
source_python(file.path(fp_github, 'fwe/USEEIO/eeio_lcia.py'))

# The model is already built so we don't need to build it again. 
# All we need to do is match the demand vector 6 digit codes with the codes that include the full names
# then run eeio_lcia on it.

demand_vector <- reduction_byfoodtype %>%
  left_join(all_codes, by = c('BEA_Code' = 'sector_code_uppercase')) %>%
  with(list(codes = sector_desc_drc, values = cost_averted))

eeio_wta <- eeio_lcia('USEEIO2012', demand_vector$values * 1e6, demand_vector$codes) 

# Also do for the baseline
baseline_byfoodtype <- data.frame(BEA_Code = row.names(food_U),
                                   baseline = rowSums(food_U))

demand_vector_baseline <- baseline_byfoodtype %>%
  left_join(all_codes, by = c('BEA_Code' = 'sector_code_uppercase')) %>%
  with(list(codes = sector_desc_drc, values = baseline))

eeio_wta_baseline <- eeio_lcia('USEEIO2012', demand_vector_baseline$values * 1e6, demand_vector_baseline$codes) 

```

This table shows the baseline impact from food produced by primary food industries that is sold to the foodservice industry. The impact averted column shows how much of this impact would be reduced if all eligible establishments in the foodservice industry implemented waste tracking and analytics software, also expressed as a percent of the original value.

```{r EEIO result as table}
# A table is probably the best way to show it
eeio_dat <- data.frame(category = row.names(eeio_wta),
                       baseline = eeio_wta_baseline$Total,
                       impact_averted = eeio_wta$Total)

eeio_dat %>%
  filter(grepl('enrg|eutr|gcc|land|watr', category)) %>%
  mutate(baseline = baseline * c(1e-9, 1e-6, 1e-9, 1e-10, 1e-9),
         impact_averted = impact_averted * c(1e-9, 1e-6, 1e-9, 1e-10, 1e-9),
         category = c('energy (PJ)', 'eutrophication (kT N)', 'greenhouse gas (MT CO2)', 'land (Mha)', 'water (km3)'),
         percent_averted = signif(100 * impact_averted/baseline, 2))
```


## 8. Estimate environmental impact of new equipment associated with intervention

Installing waste tracking and analytics systems would require each establishment to install some new equipment, consisting of scales integrated with a computer terminal where employees can record information about discarded food using a special interface. As mentioned above, we are assuming that the wholesale cost of this equipment is about 25% of the initial cost incurred by establishments adopting the intervention. Using the annualization numbers also mentioned above, this results in lower and upper bounds of \$375 and \$800 per year for equipment costs. 

We are assuming that the impact can be attributed to a combination of three BEA industry codes: (1) computers, (2) computer terminals and other computer peripheral equipment, and (3) a general-purpose machinery industry code that includes scales and balances. Because we do not know the relative proportion of impact associated with each of these industries, we take the most conservative approach possible and find the minimum and maximum possible impact across any combination of those three industries and the lower and upper bounds of the cost estimate. The general-purpose machinery code tends to have higher per-dollar impacts because it includes heavy machinery, which may not accurately reflect the environmental impact of producing retail-style scales.

However, the impacts of producing the additional required equipment are very small compared to the potential impact averted by reduced food production. For instance, the CO~2~ emissions associated with the equipment production are between 0.1% and 1.6% of the emissions averted by reduced food production. Therefore, this offset does not dramatically affect the final result; this is relatively insensitive to our assumptions. As seen in the table, the net percent impact averted, relative to baseline for the foodservice industry, after accounting for the offset is still around 9% for all impact categories.

```{r estimate environmental benefit offset}
# Number of establishments that need to adopt intervention
n_estab <- sum(establishments_affected$establishments[establishments_affected$use])

# The three potential BEA industry codes to assign costs of equipment are:
# computer manufacturing, computer monitor and peripheral manufacturing, and the other machinery category which includes industrial and retail scales
industries_offset <- c('334111', '33411A', '33399A')
total_equipment_cost <- equipment_costs * n_estab # 167M to 357M

# Find the full names of the codes for the offsetting industries
industries_offset_codes <- all_codes$sector_desc_drc[match(industries_offset, all_codes$sector_code_uppercase)]

# Use the upper and lower bounds from the industries, and the upper and lower bounds for total cost, to get an upper and lower bound for the impact that is offset

eeio_offsets_lower <- map(industries_offset_codes, ~ eeio_lcia('USEEIO2012', list(total_equipment_cost['lower']), list(.)))
eeio_offsets_upper <- map(industries_offset_codes, ~ eeio_lcia('USEEIO2012', list(total_equipment_cost['upper']), list(.)))

eeio_offsets_df <- rbind(data.frame(category = row.names(eeio_offsets_lower[[1]]), do.call(cbind, eeio_offsets_lower) %>% setNames(industries_offset_codes), bound = 'lower'),
                         data.frame(category = row.names(eeio_offsets_upper[[1]]), do.call(cbind, eeio_offsets_upper) %>% setNames(industries_offset_codes), bound = 'upper'))

eeio_offsets_range <- eeio_offsets_df %>%
  pivot_longer(cols = -c(category, bound)) %>%
  group_by(category) %>%
  group_modify(~ data.frame(offset_lower = min(.$value), offset_upper = max(.$value)))

# Final result with offset
eeio_dat_with_offset <- eeio_dat %>%
  left_join(eeio_offsets_range) %>%
  mutate(net_impact_averted_lower = impact_averted - offset_upper,
         net_impact_averted_upper = impact_averted - offset_lower) %>%
  filter(grepl('enrg|eutr|gcc|land|watr', category)) 

# For display
eeio_dat_with_offset %>%
  mutate_at(vars(baseline:net_impact_averted_upper), ~ . * c(1e-9, 1e-6, 1e-9, 1e-10, 1e-9)) %>%
  mutate(net_percent_averted_lower = 100 * net_impact_averted_lower/baseline,
         net_percent_averted_upper = 100 * net_impact_averted_upper/baseline,
         category = c('energy (PJ)', 'eutrophication (kT N)', 'greenhouse gas (MT CO2)', 'land (Mha)', 'water (km3)')) %>%
  select(-baseline, -impact_averted) %>%
  mutate_if(is.numeric, ~ signif(., 3))

```


## 8. Compare cost of implementation with environmental benefit

Because this is the first intervention we have analyzed in detail, we cannot compare the cost per unit impact reduction with other interventions yet. However, we can estimate the cost of reducing unit impact given the lower and upper bounds of the cost estimate, and compare it to existing benchmarks. For example, in the case of CO~2~ emissions, we can compare our cost to the "social cost of carbon" or the "McKinsey curve."

To get the total cost of implementation, we need to multiply the per-establishment cost by the number of establishments, across all industries, again ignoring the ineligible ones with fewer than 20 employees.

The final estimate is now based on the net environmental impact after deducting the impact of installing new equipment.

```{r multiply cost by number of establishments}
# Print table
establishments_affected %>% 
  ungroup %>%
  filter(use) %>%
  select(-use) %>%
  mutate(BEA_Title = trunc_ellipsis(BEA_Title, 30))

total_cost <- cost_range * n_estab # 4 to 9 billion

```

Given the numbers of establishments in this table, we multiply the total number of establishments (`r as.integer(n_estab)`) by the upper and lower bounds of total cost, \$8,749 and \$20,102, resulting in a total cost range of roughly **4 to 9 billion USD** to implement waste tracking and analytics across all eligible establishments.

Now we simply divide the total cost by the impact reductions to get a cost per unit reduction for each category.

```{r total cost per impact reduction}
cost_per_impact <- eeio_dat_with_offset %>%
  filter(grepl('enrg|eutr|gcc|land|watr', category)) %>%
  mutate(cost_per_reduction_lower = total_cost[1] / net_impact_averted_upper,
         cost_per_reduction_upper = total_cost[2] / net_impact_averted_lower)

cost_per_impact %>%
  select(-impact_averted, -baseline) %>%
  mutate(category = c('energy ($/MJ)', 'eutrophication ($/kg N)', 'greenhouse gas ($/kg CO2)', 'land ($/m2)', 'water ($/m3)'),
         cost_per_reduction_lower = paste0('$', round(cost_per_reduction_lower, 2)),
         cost_per_reduction_upper = paste0('$', round(cost_per_reduction_upper, 2)))
```

The final result is shown in this table. The CO~2~ emission reduction cost can be compared with the "social cost of carbon" fixed several years ago by the Obama administration at approximately 5 cents per kg CO~2~. If we consider that to be a rough benchmark for the cost-effectiveness of emissions reduction efforts, this analysis unfortunately shows a fairly high cost of CO~2~ emission reduction, ranging from \$0.44 to \$1.02. 

I will examine the assumptions here to see why the cost is so high. When I did a quick version of this analysis using the cost numbers given by ReFED, I get a much cheaper cost per kg CO~2~ reduction (\$0.03 for institutions and \$0.13 for restaurants), which may be due to the very low cost assumptions they make, or failure to account for all costs. Also, of course, CO~2~ emission reduction is not the only benefit provided by the intervention. Reducing food purchases by the foodservice industry would also reduce the other environmental impacts of food production, especially land use (which also would indirectly reduce land use related emissions of CO~2~). *Later if we can get values similar to the social cost of carbon for other environmental goods, we could sum them up to get a total value of benefit per cost &mdash; but this gets into the valuation issue which some people find problematic.*

Finally, I only account for the environmental impacts generated by the production of the raw materials up to the point where they are purchased by the foodservice industries. There are additional impacts generated when the food is transported and processed after that, that I did not consider. (However, production is by far the stage with the highest direct impact so this is unlikely to affect the final result drastically.)

Additionally, it's important to note that the environmental benefits viewed very narrowly here are not the only benefits to consider. However it may simply be that if CO~2~ is the only benefit we care about, this intervention may not be cost-effective. Once we analyze other interventions that we can compare to this one, the picture will become clearer.

## 9. Uncertainty analysis

**Note: work in progress...**

As briefly discussed above, there are a number of sources of uncertainty we can account for. On the cost side, we already have lower and upper bounds that differ by a factor of just over two. So the upper and lower bounds for our cost per unit impact reduction estimate are entirely based on that uncertainty (hence why they differ by the same factor). 

The decision of which industries to include and exclude would affect the total result but probably not affect the cost per unit impact result very much. The per-unit cost is fairly constant across the foodservice industries because they all have roughly the same baseline waste rate (from 22% to 25%) and therefore the same potential benefit from adopting the intervention. 

Changing the threshold from firms with 20 employees to a smaller threshold would increase the total cost and total benefit. It would probably increase the cost more than the benefit because the new firms being added are small; it would cost the same to apply the intervention at those small establishments, but only have a small benefit compared to intervening at large establishments.

Two other inclusion/exclusion decisions that might affect the result are (1) that I excluded government foodservice which might have a large effect since the government is a large purchaser of food for foodservice. It is known that some government foodservice establishments have already adopted waste tracking systems, meaning that there is a large potential for food waste reduction if government foodservice would uniformly adopt the intervention. SUSB lacks data on the number of establishments in government foodservice. **Is there a way to get these data?** And (2) we are not including beverages which are a fairly large contributor to foodservice environmental impact. I based that decision on my impression that these waste tracking systems cannot do a lot about beverage waste since little of it is kitchen waste, but if establishments are overpurchasing beverages that are not used, it might be addressable by the intervention.

The most important assumption to be examined is the potential effectiveness of the intervention. We used 45% reduction in kitchen waste as the number. The benefit would scale linearly with any change in this number. Therefore if the reduction achieved is only half that, the cost per unit impact reduction would approximately double. (The same is true for changes in the ratio of kitchen to plate waste but that number is more constrained by data so we can be more confident in it.)

For now we can present the uncertainty based on the cost uncertainties, but it might also be a good idea to present the uncertainty based on not knowing the true potential effectiveness of the intervention. It would be helpful to have better data on the rates of waste reduction achieved by establishments that adopt waste tracking and analytics.

# Future to-do items

* ~~Include the offsetting impacts required to produce the equipment needed for the interventions.~~ *done Feb. 19*
* Find number of government establishments so we can include that.
* Do uncertainty analysis for different ranges of assumptions.


